{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.155249,
     "end_time": "2022-03-07T15:48:59.904477",
     "exception": false,
     "start_time": "2022-03-07T15:48:59.749228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Russian River Step 2 -- simulation geometry \n",
    "\n",
    "In this step we will form the map-view simulation geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13880ecd-7fba-4d49-ac3a-dfd1d563c47a",
   "metadata": {
    "papermill": {
     "duration": 0.162792,
     "end_time": "2022-03-07T15:49:00.201712",
     "exception": false,
     "start_time": "2022-03-07T15:49:00.038920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these can be turned on for development work\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc581b68-0592-42aa-8bee-71a913ca7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1705a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.config\n",
    "import watershed_workflow.sources\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.regions\n",
    "import watershed_workflow.resampling\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9c839-7706-4428-8976-d90c9064afd0",
   "metadata": {},
   "source": [
    "## Input: Parameters and other source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c884a-be3a-4f34-bd1c-ec5c605bcd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "assert cwd[-1] == 'workflow'\n",
    "cwd = cwd[:-1]\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "output_filenames = dict()\n",
    "def fromOutput(filename):\n",
    "    return os.path.join(output_dir, filename)    \n",
    "\n",
    "def toOutput(role, filename):\n",
    "    output_filenames[role] = filename\n",
    "    return fromOutput(filename)\n",
    "\n",
    "# check output and input dirs exist\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aad6a-b0cb-4a23-8f3c-625191b2a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "# REMOVE THIS CELL for general use outside fo Coweeta\n",
    "watershed_workflow.config.setDataDirectory(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403772e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'RussianRiver'\n",
    "hucs = ['18010110'] # a list of HUCs to run\n",
    "\n",
    "# Geometric parameters\n",
    "# -- parameters to clean and reduce the river network prior to meshing\n",
    "prune_by_area = 20               # km^2\n",
    "simplify = 200                   # length scale to target average edge \n",
    "\n",
    "# -- mesh triangle refinement control\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 200\n",
    "refine_L1 = 500\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n",
    "\n",
    "# smooth angles\n",
    "min_angle = 20\n",
    "\n",
    "\n",
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.default_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58d0ca-c8c3-43e4-ba36-368c9874899c",
   "metadata": {},
   "source": [
    "# Load data from previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73eb5a-2330-42be-9ab7-8137bdb2784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_polys = gpd.read_parquet(fromOutput('01_watershed_polys.parquet'))\n",
    "reaches = gpd.read_parquet(fromOutput('01_rivers.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3007ce00-7ffa-402a-9a46-2cf1df4eaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8f94d-d370-4229-9748-b92bcd2fba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove a few points that cause trouble later...\n",
    "def removeSecondToLastCoordinate(reach_id):\n",
    "    bad = reaches.index[reaches['ID'] == reach_id].values[0]\n",
    "    \n",
    "    old_ls = list(reaches.loc[bad, 'geometry'].coords)\n",
    "    new_ls = shapely.geometry.LineString(old_ls[0:-2] + [old_ls[-1],])\n",
    "    reaches.loc[bad, 'geometry'] = new_ls\n",
    "\n",
    "removeSecondToLastCoordinate('8271033')\n",
    "removeSecondToLastCoordinate('8272363')\n",
    "removeSecondToLastCoordinate('8272363') # intentionally called twice\n",
    "\n",
    "\n",
    "#removeSecondToLastCoordinate('8272379')\n",
    "#removeSecondToLastCoordinate('8270663')\n",
    "#removeSecondToLastCoordinate('8269099')\n",
    "#removeSecondToLastCoordinate('8269105')\n",
    "#removeSecondToLastCoordinate('8270661')\n",
    "#removeSecondToLastCoordinate('8273681')\n",
    "#removeSecondToLastCoordinate('8273681') # intentionally called twice\n",
    "#removeSecondToLastCoordinate('8273271')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bca493-866c-41b3-9bd7-2952452e0220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watershed_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269093ee-ccc4-443d-a53a-e40079233bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reaches))\n",
    "rivers = watershed_workflow.river_tree.createRivers(reaches, method='native')\n",
    "print(len(rivers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708cf9c7-85c9-496d-b731-cd15b45e4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark the outlet reaches as do-not-merge\n",
    "for river in rivers:\n",
    "    river.df['do-not-merge'] = [0,] * len(river)\n",
    "\n",
    "roots = [rivers[watershed_polys.loc[index, 'river_index']].getNode(watershed_polys.loc[index, 'reach_index'])\n",
    "             for index in watershed_polys.index]\n",
    "for outlet, root in zip(watershed_polys.index, roots):\n",
    "    if watershed_polys.loc[outlet].location_on_reach == 0:\n",
    "        root['do-not-merge'] = -1\n",
    "    elif watershed_polys.loc[outlet].location_on_reach == 1:\n",
    "        root['do-not-merge'] = 1\n",
    "    else:\n",
    "        assert False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d6f1d-a05c-49c4-b0bb-10f2ea169800",
   "metadata": {},
   "source": [
    "## Compute disjoint subcatchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4d268-9723-4916-9d9c-0176f1e42ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "watersheds = watershed_workflow.split_hucs.SplitHUCs(watershed_polys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba669c-8214-4265-bb99-1221392c5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates a zoomable map, showing different reaches and watersheds, \n",
    "# with discrete points.  Problem areas are clickable to get IDs for manual\n",
    "# modifications.\n",
    "m = watersheds.explore(marker=True, marker_size=10)\n",
    "\n",
    "for river in rivers:\n",
    "    m = river.explore(column=None, m=m, color='black', name=river['name'], marker=True, marker_size=10)\n",
    "\n",
    "#m = gpd.GeoDataFrame(geometry=[shapely.geometry.Point(point)], crs=watersheds.crs).explore(m=m, color='k', marker_size=100)\n",
    "\n",
    "m = watershed_workflow.makeMap(m)\n",
    "m\n",
    "\n",
    "#fig, ax = plt.subplots(1,1)\n",
    "#watersheds.plot(ax=ax, color='k')\n",
    "#rivers[0].plot(ax=ax, color='b')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e798070-4498-43f1-8a25-2c85bd808f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bad points on watersheds\n",
    "old_ls = list(watersheds.linestrings[30].coords)\n",
    "new_ls = shapely.geometry.LineString(old_ls[0:80]+old_ls[85:])\n",
    "watersheds.linestrings[30] = new_ls\n",
    "\n",
    "\n",
    "old_ls = list(watersheds.linestrings[44].coords)\n",
    "new_ls = shapely.geometry.LineString(old_ls[0:96]+old_ls[97:])\n",
    "watersheds.linestrings[44] = new_ls\n",
    "\n",
    "# must update as we have modified geometry\n",
    "watersheds.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697f59d-4881-4549-996c-a68609389e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simplifying \n",
    "watershed_workflow.simplify(watersheds, rivers, \n",
    "                            reach_segment_target_length=refine_L0,\n",
    "                            huc_segment_target_length=refine_L1,\n",
    "                            river_close_distance=refine_d0,\n",
    "                            river_far_distance=refine_d1,\n",
    "                            snap_triple_junctions_tol=refine_L0,\n",
    "                            min_angle=min_angle,\n",
    "                            junction_min_angle=min_angle\n",
    "                           )\n",
    "\n",
    "# greatly shrunk the rivers... shrink the dataframe too\n",
    "for river in rivers:\n",
    "    river.resetDataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fceebf-9704-4666-bc7a-0262ce78e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "watersheds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e8459-7275-44d9-a07d-a99be33df8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates a zoomable map, showing different reaches and watersheds, \n",
    "# with discrete points.  Problem areas are clickable to get IDs for manual\n",
    "# modifications.\n",
    "m = watersheds.explore(marker=True, marker_size=10)\n",
    "\n",
    "for river in rivers:\n",
    "    m = river.explore(column=None, m=m, color='black', name=river['name'], marker=True, marker_size=10)\n",
    "\n",
    "m = watershed_workflow.makeMap(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e40d00-9e69-499e-bc3e-5b01dac85e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move one last point... Santa Rosa does weird stuff...\n",
    "old_ls = list(watersheds.linestrings[57].coords)\n",
    "new_ls = shapely.geometry.LineString(old_ls[:-1] + [list(watersheds.linestrings[62].coords)[7],])\n",
    "watersheds.linestrings[57] = new_ls\n",
    "\n",
    "watersheds.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73ee98-fd9a-4823-9edd-b0ba2c7bfeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this final version of rivers and watersheds to disk\n",
    "import pickle\n",
    "with open(toOutput('watershed_polys', '02_watersheds.pickle'), 'wb') as fid:\n",
    "    pickle.dump(watersheds, fid)\n",
    "\n",
    "river_df = gpd.GeoDataFrame(pd.concat([r.to_dataframe() for r in rivers]), crs=crs)\n",
    "river_df.to_parquet(toOutput('rivers', '02_rivers.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1afb4f-f620-4230-84a2-ed02551a61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output filenames\n",
    "with open(toOutput('02_output_filenames', '02_output_filenames.txt'), 'wb') as fid:\n",
    "    pickle.dump(output_filenames, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab17132-9db9-4c5f-931b-77bd99505695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:ww-geopandas-20250725]",
   "language": "python",
   "name": "conda-env-ww-geopandas-20250725-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.28134,
   "end_time": "2022-03-07T16:00:23.770205",
   "environment_variables": {},
   "exception": true,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.005
   },
   "start_time": "2022-03-07T15:48:58.488865",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
