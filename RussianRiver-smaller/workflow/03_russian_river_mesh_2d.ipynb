{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2685730-fb44-4e19-8d1e-2e9760bb0290",
   "metadata": {},
   "source": [
    "# Russian River Step 3 -- 2D mesh\n",
    "\n",
    "Form the 2D mesh, elevate via a DEM, condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fbadbc-e7fd-4e27-ac5b-58e7bfa4edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f8987-1355-460f-8e33-17cac1ff55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b02b-0e2e-4153-9923-441883abf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.config\n",
    "import watershed_workflow.sources\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.regions\n",
    "import watershed_workflow.condition\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c07d96-5ef2-4d36-a9d3-2b03c2db2fd1",
   "metadata": {},
   "source": [
    "## Input: Parameters and other source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30623be6-942c-4791-b3c5-fea447b4c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "assert cwd[-1] == 'workflow'\n",
    "cwd = cwd[:-1]\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "output_filenames = dict()\n",
    "def fromOutput(filename):\n",
    "    return os.path.join(output_dir, filename)    \n",
    "\n",
    "def toOutput(role, filename):\n",
    "    output_filenames[role] = filename\n",
    "    return fromOutput(filename)\n",
    "\n",
    "# check output and input dirs exist\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f2a3f-2a44-44e4-8715-fe7cc573e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "# REMOVE THIS CELL for general use outside fo Coweeta\n",
    "watershed_workflow.config.setDataDirectory(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a07995-f605-4fde-9bab-2164a7b256eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'RussianRiver'\n",
    "hucs = ['18010110'] # a list of HUCs to run\n",
    "\n",
    "\n",
    "# -- parameters to clean and reduce the river network prior to meshing\n",
    "prune_by_area = 20               # km^2\n",
    "simplify = 200                   # length scale to target average edge \n",
    "\n",
    "# -- mesh triangle refinement control\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 200\n",
    "refine_L1 = 500\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n",
    "\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 20 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "river_widths = dict({1:10, 2:10, 3:20, 4:30, 5:30}) \n",
    "\n",
    "\n",
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.default_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc11ec-b8e4-4c69-9cb8-40c8189a5d8f",
   "metadata": {},
   "source": [
    "## Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04a47b-541a-48fc-97ed-703275de3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fromOutput('02_watersheds.pickle'), 'rb') as fid:\n",
    "    watersheds = pickle.load(fid)\n",
    "\n",
    "reaches = gpd.read_parquet(fromOutput('02_rivers.parquet'))\n",
    "rivers = watershed_workflow.river_tree.createRivers(reaches, method='native')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426719a-12b1-4901-a709-13f7160085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates a zoomable map, showing different reaches and watersheds, \n",
    "# with discrete points.  Problem areas are clickable to get IDs for manual\n",
    "# modifications.\n",
    "m = watersheds.explore(marker=True, marker_size=10)\n",
    "\n",
    "for river in rivers:\n",
    "    m = river.explore(m=m, color='black', name=river['name'], marker=True, marker_size=10)\n",
    "\n",
    "m = watershed_workflow.makeMap(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08decdc-b109-47e0-9b81-f16017a0d488",
   "metadata": {},
   "source": [
    "## Generate the mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383600e2-fe53-4fa3-b42a-aee8422864bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2, areas, dists = watershed_workflow.tessalateRiverAligned(watersheds, rivers, river_width = river_widths,\n",
    "                                             refine_min_angle = min_angle, refine_distance = [refine_d0, refine_A0, refine_d1, refine_A1],\n",
    "                                             diagnostics=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26772e21-eefe-4da1-a411-dd9ca24c489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-partition the mesh \n",
    "#print(m2.num_cells)\n",
    "#print(m2.num_cells * 10 / 4000)\n",
    "#\n",
    "## let's use 192, 3 * 64\n",
    "#m2 = m2.partition(192, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c223b1f-9c5d-4ea6-94a1-d9bc0ab36b79",
   "metadata": {},
   "source": [
    "## Get a DEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccd416-eca4-4a02-bb04-2c7749b39681",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = watershed_workflow.sources.dem_sources['3DEP 30m'].getDataset(watersheds.df)['dem']\n",
    "\n",
    "# provide surface mesh elevations\n",
    "watershed_workflow.elevate(m2, dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434060c-809d-4183-a2d2-efa93c53f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits\n",
    "river_mask = np.zeros((len(m2.conn)))\n",
    "for i, elem in enumerate(m2.conn):\n",
    "    if not len(elem) == 3:\n",
    "        river_mask[i] = 1     \n",
    "watershed_workflow.condition.fillPitsDual(m2, is_waterbody=river_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a8bbe2-3670-4436-8dc1-76103c2da129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DEM raster\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot the DEM data\n",
    "im = dem.plot(ax=ax, cmap='terrain', add_colorbar=False)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('Elevation (m)', rotation=270, labelpad=15)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Digital Elevation Model (DEM)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "# Set equal aspect ratio\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc264dbe-3284-4a3d-a3ad-60df804364ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.labeled_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57b2d1-bb40-43fb-9dda-7ac61e2093e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(watersheds.df.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ab82f-9a0b-4074-b057-c9ec2d258b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add regions for each polygon\n",
    "watershed_workflow.regions.addWatershedAndOutletRegions(m2, watersheds, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe11fe-7285-440d-877b-d9a9d1a56cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add regions for each stream order\n",
    "watershed_workflow.regions.addStreamOrderRegions(m2, rivers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c566e8-349b-4303-bee4-4c1ea1a1edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRed(txt):\n",
    "    print(\"\\033[31m\", txt, \"\\033[0m\")\n",
    "\n",
    "print('2D labeled sets')\n",
    "print('---------------')\n",
    "for ls in m2.labeled_sets:\n",
    "    printer = print\n",
    "    if len(ls.ent_ids) == 0:\n",
    "        printer = printRed\n",
    "    printer(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7cf6c-938b-4767-a2c7-91ffc07ab5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mesh and regions\n",
    "with open(toOutput('m2', '03_m2.pickle'), 'wb') as fid:\n",
    "    pickle.dump(m2, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8fc33e-ac7d-4682-8487-5f4198d02b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly, reread, update, and output filenames\n",
    "with open(toOutput('03_output_filenames', '03_output_filenames.txt'), 'wb') as fid:\n",
    "    pickle.dump(output_filenames, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800b654-7ed7-418b-a51b-a201298cd20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29a8f3-8891-4dd4-9d75-c23d156ddbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d43c57-9a29-4a52-abf6-3897bf7501a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ww-geopandas-20250725]",
   "language": "python",
   "name": "conda-env-ww-geopandas-20250725-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
