{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.155249,
     "end_time": "2022-03-07T15:48:59.904477",
     "exception": false,
     "start_time": "2022-03-07T15:48:59.749228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Russian River Step 1 -- download and clean raw geometry data\n",
    "\n",
    "In this section, we choose the basin, the streams to be included in the stream-aligned mesh, and make sure that all are resolved discretely at appropriate length scales for this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13880ecd-7fba-4d49-ac3a-dfd1d563c47a",
   "metadata": {
    "papermill": {
     "duration": 0.162792,
     "end_time": "2022-03-07T15:49:00.201712",
     "exception": false,
     "start_time": "2022-03-07T15:49:00.038920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these can be turned on for development work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc581b68-0592-42aa-8bee-71a913ca7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1705a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "pd.options.display.max_columns = None\n",
    "import pickle\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.config\n",
    "import watershed_workflow.sources\n",
    "import watershed_workflow.sources.standard_names as names\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9c839-7706-4428-8976-d90c9064afd0",
   "metadata": {},
   "source": [
    "## Input: Parameters and other source data\n",
    "\n",
    "Note, this section will need to be modified for other runs of this workflow in other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c884a-be3a-4f34-bd1c-ec5c605bcd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "assert cwd[-1] == 'workflow'\n",
    "cwd = cwd[:-1]\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "output_filenames = dict()\n",
    "def fromOutput(filename):\n",
    "    return os.path.join(output_dir, filename)    \n",
    "\n",
    "def toOutput(role, filename):\n",
    "    output_filenames[role] = filename\n",
    "    return fromOutput(filename)\n",
    "\n",
    "# check output and input dirs exist\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aad6a-b0cb-4a23-8f3c-625191b2a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "watershed_workflow.config.setDataDirectory(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403772e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'RussianRiver'\n",
    "hucs = ['18010110'] # a list of HUCs to run\n",
    "\n",
    "# Geometric parameters\n",
    "# -- parameters to clean and reduce the river network prior to meshing\n",
    "prune_by_area = 10               # km^2\n",
    "simplify = 125                   # length scale to target average edge \n",
    "\n",
    "# -- mesh triangle refinement control\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 125\n",
    "refine_L1 = 300\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n",
    "\n",
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.default_crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17feb6e0",
   "metadata": {
    "papermill": {
     "duration": 0.181137,
     "end_time": "2022-03-07T15:49:06.850648",
     "exception": false,
     "start_time": "2022-03-07T15:49:06.669511",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "#\n",
    "# Data sources, also called managers, deal with downloading and parsing data files from a variety of online APIs.\n",
    "sources = watershed_workflow.sources.getDefaultSources()\n",
    "\n",
    "# log the sources that will be used here\n",
    "watershed_workflow.sources.logSources(sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05763672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape and crs of the shape\n",
    "print(crs)\n",
    "watershed_shapes = sources['HUC'].getShapesByID(hucs, out_crs=crs)\n",
    "print(watershed_shapes.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208035c-0731-4537-ad22-a7119e1c3a77",
   "metadata": {},
   "source": [
    "## the Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct and plot the WW object used for storing watersheds\n",
    "watershed = watershed_workflow.split_hucs.SplitHUCs(watershed_shapes)\n",
    "watershed.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcc6e6-a50d-4dff-a6ec-d3f42246bd19",
   "metadata": {},
   "source": [
    "## Gage Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3fa08-3a82-4561-bf52-86d8d1c83501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all gages in the river\n",
    "import pygeohydro\n",
    "\n",
    "# Initialize NWIS\n",
    "nwis = pygeohydro.NWIS()\n",
    "bbox = ','.join(f\"{b:.06f}\" for b in watershed_shapes.to_crs(watershed_workflow.crs.latlon_crs).geometry[0].bounds)\n",
    "print(bbox)\n",
    "\n",
    "query = {\n",
    "    \"bBox\": bbox,\n",
    "    \"siteType\": \"ST\",  # stream types\n",
    "    \"parameterCd\": \"00060,00065\", # Discharge and Gage height\n",
    "    \"hasDataTypeCd\": \"dv\",  # Daily values\n",
    "    \"outputDataTypeCd\": \"dv\",  # Output as daily values\n",
    "}\n",
    "\n",
    "#\n",
    "# Fetch all available gages within the bounding box\n",
    "sites = nwis.get_info(query).to_crs(crs)\n",
    "\n",
    "#\n",
    "# Spatial join: keep only sites that fall inside the watershed, not the bounds\n",
    "sites = gpd.sjoin(sites, watershed_shapes, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "#\n",
    "# limit to sites with end_date after 2000 -- they MIGHT have good data\n",
    "sites = sites[sites['end_date'] > '2000-01-01']\n",
    "\n",
    "sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc97c9a-8716-4489-8758-a62c3f0d2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# do they actually have data?  download discharge\n",
    "#\n",
    "dates = ('2000-01-01', '2026-01-01')\n",
    "qobs = nwis.get_streamflow(sites.site_no.to_list(), dates, mmd=True)\n",
    "qobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdd049-5d07-4275-b1b9-09bf1747934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 36 have data... how many have lots of data?  Lets say at least 10 years of daily data since 2000...\n",
    "#\n",
    "qobs_10yrs_k = [k for k in qobs.keys() if qobs[k].count() > 10*365]\n",
    "len(qobs_10yrs_k)\n",
    "\n",
    "# these are the 24 that we will mesh into the domain...\n",
    "qobs_10yrs = qobs[qobs_10yrs_k]\n",
    "qobs_10yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95c441-acd3-4beb-b645-5a7d7fc63038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create a new sites dataframe, that just has the metadata of continuous sites, and not repeated site_nos associated with different ranges\n",
    "#\n",
    "geom = [shapely.geometry.Point(qobs_10yrs.attrs[k]['dec_long_va'], qobs_10yrs.attrs[k]['dec_lat_va']) for k in qobs_10yrs.keys()] \n",
    "sites_10yrs = gpd.GeoDataFrame(geometry=geom, crs=watershed_workflow.crs.latlon_crs)\n",
    "\n",
    "cols = qobs_10yrs.attrs[list(qobs_10yrs.keys())[0]].keys()\n",
    "for col in cols:\n",
    "    vals = [qobs_10yrs.attrs[k][col] for k in qobs_10yrs.keys()]\n",
    "    sites_10yrs[col] = vals    \n",
    "\n",
    "sites_10yrs['ID'] = list(qobs_10yrs.keys())\n",
    "sites_10yrs['count'] = [qobs_10yrs[k].count() for k in qobs_10yrs.keys()]\n",
    "sites_10yrs = sites_10yrs.to_crs(crs)\n",
    "sites_10yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c02c1-1bd2-4354-8b32-60a83ae2d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynhd\n",
    "nldi = pynhd.NLDI()\n",
    "\n",
    "features = nldi.getfeature_byid(\"nwissite\", sites_10yrs['ID'])\n",
    "\n",
    "# pull a few things over from the nwis data\n",
    "sites_10yrs['comid'] = features['comid'].astype(int)\n",
    "sites_10yrs['measure'] = features['measure'] # in % of total length, the distance up the reach from the downstream point to the gage\n",
    "sites_10yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e02e-b4c7-49d1-b0b0-090928ff2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that no two gages are on the same reach -- this would break our subdomain decomposition\n",
    "assert len(set(sites_10yrs.comid.tolist())) == len(sites_10yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c4bd2-cfce-45a8-82e7-06ff7e7f3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are pairs of points that are super close\n",
    "\n",
    "def checkTooClose(pts, geom='geometry'):\n",
    "    pairs_dist = []\n",
    "    pairs_comid = []\n",
    "    threshold = 1000 # m\n",
    "    \n",
    "    for i, geom1 in enumerate(pts[geom]):\n",
    "        for j, geom2 in enumerate(pts[geom]):\n",
    "            if i < j:\n",
    "                d = geom1.distance(geom2)\n",
    "                if d <= threshold:\n",
    "                    pairs_dist.append((pts.index[i], pts.index[j], d))\n",
    "    \n",
    "                if pts.iloc[i].comid == pts.iloc[j].comid:\n",
    "                    pairs_comid.append((pts.index[i], pts.index[j], pts.iloc[j].comid))\n",
    "    \n",
    "    print('Close pairs:')\n",
    "    for i,j,d in pairs_dist:\n",
    "        print(f'point {i} ({pts.loc[i, geom]}) and point {j} {pts.loc[j,geom]}) are within {d} m')\n",
    "    \n",
    "    print('Shared comid:')\n",
    "    for i,j,c in pairs_comid:\n",
    "        print(f'point {i} ({pts.loc[i, geom]}) and point {j} {pts.loc[j,geom]}) share comid {c}')\n",
    "    return pairs_dist, pairs_comid\n",
    "\n",
    "pairs_dist, _ = checkTooClose(sites_10yrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddbb06-d1b2-441f-bd2a-4edecf4b28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove the one with fewer records:\n",
    "for i,j,d in pairs_dist:\n",
    "    fewer = i if sites_10yrs.loc[i, 'count'] < sites_10yrs.loc[j, 'count'] else j\n",
    "\n",
    "sites_10yrs = sites_10yrs.drop(fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553093d3-ef8b-4d81-9cc1-73069d0192be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# After going through this once, two other gages -- \n",
    "#   USGS-11463980\n",
    "#   USGS-11464000\n",
    "# are rather close to each other, and result in a small subcatchment between them, where the \n",
    "# subcatchments downstream of 11464000 and upstream of 11463980 touch each other on either\n",
    "# side of the small subcatchment, which breaks the partitioning algorithm.\n",
    "#\n",
    "# We'll remove the upstream one, as it has fewer records.\n",
    "sites_10yrs = sites_10yrs[sites_10yrs['ID'] != 'USGS-11463980']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a65bd",
   "metadata": {},
   "source": [
    "## the Rivers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5828ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download/collect the river network within that shape's bounds\n",
    "reaches = sources['hydrography'].getShapesByGeometry(watershed.df, out_crs=crs)\n",
    "\n",
    "# remove coastlines\n",
    "reaches = reaches[reaches.ftype != 'Coastline']\n",
    "print(reaches.crs)\n",
    "\n",
    "# construct rivers\n",
    "rivers = watershed_workflow.river_tree.createRivers(reaches, method='hydroseq')\n",
    "print(rivers[0].df.crs)\n",
    "\n",
    "reaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbbe97-9aa5-4771-a292-86e354107dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# are all of our gage-reaches in the set of reaches?\n",
    "print(sum(sites_10yrs['comid'].isin(reaches['comid'])), ' of ', len(sites_10yrs), ' are in the set of ALL reaches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd0d4b-0b62-48f1-a3ba-6da052197122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ws, rivs, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ws.plot(color='k', marker='+', markersize=10, ax=ax)\n",
    "    for river in rivs:\n",
    "        river.plot(marker='x', markersize=10, ax=ax)\n",
    "\n",
    "    return ax\n",
    "\n",
    "ax = plot(watershed, rivers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec3320-68b7-4bc7-8d7e-0e8bc55b1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outlet here needs to be modified thanks to the coastline reaches\n",
    "\n",
    "# move the endpoint to the boundary\n",
    "op = rivers[0].linestring.coords[-1]\n",
    "cp = shapely.ops.nearest_points(watershed.exterior.exterior, rivers[0].linestring)[0]\n",
    "print(op, cp)\n",
    "\n",
    "rivers[0].moveCoordinate(-1, cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f525e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune\n",
    "rivers = watershed_workflow.reduceRivers(rivers, \n",
    "                                         prune_by_area=prune_by_area,\n",
    "                                         remove_diversions=True,\n",
    "                                         remove_braided_divergences=True)\n",
    "\n",
    "for river in rivers:\n",
    "    river.resetDataFrame()\n",
    "\n",
    "reduced_reaches = pd.concat([r.df for r in rivers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe601f3b-947a-49c5-9663-1182d9c6413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# are all of our gages STILL in our reduced network?\n",
    "print(sum(sites_10yrs['comid'].isin(reduced_reaches['comid'])), ' of ', len(sites_10yrs), ' are in the set of REDUCED reaches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49788949-eee1-459b-aa95-cb59fd6c7cb5",
   "metadata": {},
   "source": [
    "## Put the gages on the reaches, and use these to define subcatchments\n",
    "\n",
    "We don't have a good way of splitting the catchment of a reach at the gage's measure of the reach.  If we had this, we could split both the reach and the catchment at the actual gage location, and make subcatchments that respect this.  Let's try to assign the gage to an upstream-most or downstream-most point on the reach instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaa10d-9afb-4a84-b382-8aa7adfc98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another \"gage\" point -- the outlet of the full domain\n",
    "assert len(rivers) == 1\n",
    "sites_10yrs = sites_10yrs.reset_index()\n",
    "\n",
    "sites = gpd.GeoDataFrame(\n",
    "    pd.concat([sites_10yrs,\n",
    "           gpd.GeoDataFrame({'comid' : rivers[0]['comid'],\n",
    "                             'measure' : 0.,\n",
    "                             'station_nm' : 'Russian River Outlet',\n",
    "                             'ID' : 'RR-outlet',\n",
    "                            },\n",
    "                            index=[len(sites_10yrs),],\n",
    "                            geometry=[shapely.geometry.Point(rivers[0].linestring.coords[-1]),],\n",
    "                            crs=sites_10yrs.crs),\n",
    "          ]), crs=sites_10yrs.crs)\n",
    "\n",
    "sites[names.NAME] = sites[names.ID]\n",
    "sites.pop('index')\n",
    "sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c40bb7-cfe2-45bf-873a-7f16ca7986a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aead9b-23ca-484f-a895-15001da10779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# first map from the site locations to the end of the reach we wish to map the gage onto\n",
    "#\n",
    "sites['reach_ID'] = sites['comid'].astype(str)\n",
    "watershed_polys = watershed_workflow.river_tree.determineOutletToReachMap(rivers, sites)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba97b9f-0b35-485f-9392-8d4f6fa8a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that moving to outlets didn't break our set of points?\n",
    "c1, c2 = checkTooClose(watershed_polys, 'true_geometry')\n",
    "assert len(c1) == 0\n",
    "assert len(c2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caaeb9-65da-45da-af3e-b23b238c5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now accumluate incremental catchments\n",
    "watershed_polys = watershed_workflow.river_tree.accumulateIncrementalCatchments(rivers, watershed_polys)\n",
    "watershed_polys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52de14-695d-4077-ad95-70e0c3387265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap some of the geometry fields to make life a little simpler\n",
    "watershed_polys['true_gage_geometry'] = watershed_polys['true_geometry']\n",
    "watershed_polys['outlet'] = watershed_polys['geometry']\n",
    "watershed_polys['geometry'] = watershed_polys['incremental_catchment']\n",
    "watershed_polys = watershed_polys.set_geometry('geometry', crs=watershed_polys.crs)\n",
    "print(watershed_polys.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3d2b6-921f-49b3-a2a6-d2a9be9a9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one multipolygon?  take the biggest subset...\n",
    "watershed_polys['geometry'] = [watershed_workflow.split_hucs.findBiggest(p.geoms) if isinstance(p, shapely.geometry.MultiPolygon) else p for p in watershed_polys.geometry]\n",
    "print(watershed_polys.crs)\n",
    "watershed_polys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08a8f8-e5e7-41c0-ae9d-0ec119887d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all area > 0?\n",
    "print(watershed_polys.area)\n",
    "assert min(watershed_polys.area) > 0.\n",
    "print(len(watershed_polys))\n",
    "print(len(sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73bdb1-088f-438e-b3f5-b990a842e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, geo_i in enumerate(watershed_polys.geometry):\n",
    "    for j, geo_j in enumerate(watershed_polys.geometry):\n",
    "        if j > i:\n",
    "            if geo_i.contains(geo_j):\n",
    "                print(f\"CONTAINS: {watershed_polys.index[i]} comid {watershed_polys.iloc[i]['comid']} contains {watershed_polys.index[j]} comid {watershed_polys.iloc[j]['comid']}\")            \n",
    "            if geo_j.contains(geo_i):\n",
    "                print(f\"CONTAINS: {watershed_polys.index[j]} comid {watershed_polys.iloc[j]['comid']} contains {watershed_polys.index[i]} comid {watershed_polys.iloc[i]['comid']}\")            \n",
    "\n",
    "            area = geo_i.intersection(geo_j).area\n",
    "            af_i = area / geo_i.area\n",
    "            af_j = area / geo_j.area\n",
    "            if af_i > .01 or af_j > .01:\n",
    "                print('large intersection:', watershed_polys.index[i], watershed_polys.index[j], 'area_frac =', af_i, af_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b5b8a-49a0-4d6b-aefb-2c4355da3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these shapefiles to disk\n",
    "watershed_polys.to_parquet(toOutput('watershed_polys', '01_watershed_polys.parquet'))\n",
    "\n",
    "river_df = gpd.GeoDataFrame(pd.concat([r.to_dataframe() for r in rivers]), crs=crs)\n",
    "river_df.to_parquet(toOutput('rivers', '01_rivers.parquet'))\n",
    "\n",
    "qobs.to_csv(toOutput('evaluation_discharge', '01_discharge_observations.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054d6d5-329d-4583-b614-3541f9f06f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output filenames\n",
    "with open(toOutput('04_output_filenames', '01_output_filenames.txt'), 'wb') as fid:\n",
    "    pickle.dump(output_filenames, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262c178-9efb-4a0e-87b6-5a8649de5075",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:ww-geopandas-20250725]",
   "language": "python",
   "name": "conda-env-ww-geopandas-20250725-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.28134,
   "end_time": "2022-03-07T16:00:23.770205",
   "environment_variables": {},
   "exception": true,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.005
   },
   "start_time": "2022-03-07T15:48:58.488865",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
