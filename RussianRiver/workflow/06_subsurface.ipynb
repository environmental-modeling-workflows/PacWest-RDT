{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be07c9-d38d-4e20-821f-354b7375c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61582f0-ffa1-488c-8897-084dd11fcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec44dd4-e68a-4670-84bf-e3d7f5c19572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b475f5e-7ae1-490c-92f0-c8f75ee55e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import cftime\n",
    "import datetime\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.config\n",
    "import watershed_workflow.sources\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.regions\n",
    "import watershed_workflow.land_cover_properties\n",
    "import watershed_workflow.io\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b382b7-d17b-4123-921a-8f3454e36417",
   "metadata": {},
   "source": [
    "## Input: Parameters and other source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8835f-533f-488e-b5be-a7788abb5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "assert cwd[-1] == 'workflow'\n",
    "cwd = cwd[:-1]\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_filenames = dict()\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "def fromOutput(filename):\n",
    "    return os.path.join(output_dir, filename)    \n",
    "\n",
    "def toOutput(role, filename):\n",
    "    output_filenames[role] = filename\n",
    "    return fromOutput(filename)\n",
    "\n",
    "# check output and input dirs exist\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b47a5-b5cf-47e4-9940-fa1106f209e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "# REMOVE THIS CELL for general use outside fo Coweeta\n",
    "watershed_workflow.config.setDataDirectory(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a439d2-ac6a-49b8-a617-5276c000902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'RussianRiver'\n",
    "hucs = ['18010110'] # a list of HUCs to run\n",
    "\n",
    "# Geometric parameters\n",
    "# -- parameters to clean and reduce the river network prior to meshing\n",
    "prune_by_area = 10               # km^2\n",
    "simplify = 125                   # length scale to target average edge \n",
    "\n",
    "# -- mesh triangle refinement control\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 125\n",
    "refine_L1 = 300\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 20 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "river_widths = dict({1:10, 2:10, 3:20, 4:30, 5:30}) \n",
    "\n",
    "\n",
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.default_crs\n",
    "\n",
    "\n",
    "# start and stop time for simulation\n",
    "# note that this is the overlap of AORC and MODIS\n",
    "start = cftime.DatetimeGregorian(2007, 8, 1)\n",
    "end = cftime.DatetimeGregorian(2020, 7, 31)\n",
    "\n",
    "start_noleap = cftime.DatetimeNoLeap(2007, 8, 1)\n",
    "end_noleap = cftime.DatetimeNoLeap(2020, 7, 31)\n",
    "cyclic_nyears = 10\n",
    "\n",
    "\n",
    "# Global Soil Properties\n",
    "min_porosity = 0.05 # minimum porosity considered \"too small\"\n",
    "max_permeability = 1.e-10 # max value considered \"too permeable\"\n",
    "max_vg_alpha = 1.e-3 # max value of van Genuchten's alpha -- our correlation is not valid for some soils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb31a8a-67c0-4d10-a9d7-e8c8689eace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "#\n",
    "# Data sources, also called managers, deal with downloading and parsing data files from a variety of online APIs.\n",
    "sources = watershed_workflow.sources.getDefaultSources()\n",
    "\n",
    "# log the sources that will be used here\n",
    "watershed_workflow.sources.logSources(sources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ee9a5-d87e-4b6e-8188-45af015a2845",
   "metadata": {},
   "source": [
    "## Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef83574-5098-46df-b721-9af64c6ad06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fromOutput('02_watersheds.pickle'), 'rb') as fid:\n",
    "    watersheds = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79172afb-df63-4662-afa5-b3a51d36eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fromOutput('04_m2.pickle'), 'rb') as fid:\n",
    "    m2 = pickle.load(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda119e-f684-44b5-814f-849780638268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2.cell_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57036b84-4394-4e0d-b5e9-701c982f7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also load a reference watershed, which will be used to download the data -- this data is already downloaded\n",
    "# so saves us from small wiggles requiring a new download\n",
    "#ref_watershed = gpd.read_parquet(fromOutput('01_reference_watershed.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5992b4-173b-429c-80a1-938edc80ab0c",
   "metadata": {},
   "source": [
    "## Subsurface Soil, Geologic Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fee2b6-b845-46cf-a05b-c02273366519",
   "metadata": {},
   "source": [
    "### NRCS Soils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9781ee-a5ff-45b9-9406-26a523ad0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NRCS shapes, on a reasonable crs\n",
    "nrcs = sources['soil structure'].getShapesByGeometry(watersheds.exterior, watersheds.crs, watersheds.crs)\n",
    "nrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8f9ea-8012-4d59-8c82-fd0530b3535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean dataframe with just the data we will need for ATS\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(nrcs, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(nrcs, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    nrcs.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "nan_mask = nrcs.isna().any(axis=1)\n",
    "dropped_mukeys = nrcs.index[nan_mask]\n",
    "\n",
    "# Drop those rows\n",
    "nrcs = nrcs[~nan_mask]\n",
    "\n",
    "assert nrcs['porosity [-]'][:].min() >= min_porosity\n",
    "assert nrcs['permeability [m^2]'][:].max() <= max_permeability\n",
    "nrcs\n",
    "\n",
    "# check for nans\n",
    "nrcs.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd5e9f-b7ab-4a28-a639-92bc1bd5c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12da55d-73ee-4303-ae44-f0291fc3114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the soil color of each cell of the mesh\n",
    "# Note, we use mukey here because it is an int, while ID is a string\n",
    "soil_color_mukey = watershed_workflow.getShapePropertiesOnMesh(m2, nrcs, 'mukey', \n",
    "                                                        resolution=50, nodata=-999)\n",
    "nrcs.set_index('mukey', drop=False, inplace=True)\n",
    "\n",
    "unique_soil_colors = list(np.unique(soil_color_mukey))\n",
    "if -999 in unique_soil_colors:\n",
    "    unique_soil_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of soil_color\n",
    "nrcs = nrcs.loc[unique_soil_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "nrcs['ATS ID'] = range(1000, 1000+len(unique_soil_colors))\n",
    "nrcs.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new soil color and a soil thickness map using the ATS IDs\n",
    "soil_color = -np.ones_like(soil_color_mukey)\n",
    "soil_thickness = np.nan * np.ones(soil_color.shape, 'd')\n",
    "\n",
    "for ats_ID, ID, thickness in zip(nrcs.index, nrcs.mukey, nrcs['thickness [m]']):\n",
    "    mask = np.where(soil_color_mukey == ID)\n",
    "    soil_thickness[mask] = thickness\n",
    "    soil_color[mask] = ats_ID\n",
    "\n",
    "m2.cell_data['soil_color'] = soil_color\n",
    "m2.cell_data['soil thickness'] = soil_thickness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493f923-70fc-48a1-8c09-7f7cfaeee1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the soil color\n",
    "# -- get a cmap for soil color\n",
    "sc_indices, sc_cmap, sc_norm, sc_ticks, sc_labels = \\\n",
    "      watershed_workflow.colors.createIndexedColormap(nrcs.index)\n",
    "\n",
    "mp = m2.plot(facecolors=m2.cell_data['soil_color'], cmap=sc_cmap, norm=sc_norm, edgecolors=None, colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nrcs), \n",
    "                               cmap=sc_cmap, labels=sc_labels, ax=plt.gca()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fe108-8b13-4021-8354-76d39f6998bd",
   "metadata": {},
   "source": [
    "### Depth to Bedrock from Pelletier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53476d42-b4d0-4202-a2ef-f16fc96b6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb = sources['depth to bedrock'].getDataset(watersheds.exterior, watersheds.crs)['band_1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cb7d1-f54e-40d3-a02c-bd13f7c84eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to the mesh\n",
    "m2.cell_data['dtb'] = watershed_workflow.getDatasetOnMesh(m2, dtb, method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d6297-bd74-4bf2-9f13-03ec19c49f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gons = m2.plot(facecolors=m2.cell_data['dtb'], cmap='RdBu', edgecolors=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab240488-d0fb-40e3-b333-e87e07253609",
   "metadata": {},
   "source": [
    "### GLHYMPs Geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e1394-6eb6-4f5d-81b1-41fe797285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps = sources['geologic structure'].getShapesByGeometry(watersheds.exterior.buffer(1000), watersheds.crs, out_crs=crs)\n",
    "glhymps = watershed_workflow.soil_properties.mangleGLHYMPSProperties(glhymps,\n",
    "                                              min_porosity=min_porosity, \n",
    "                                              max_permeability=max_permeability, \n",
    "                                              max_vg_alpha=max_vg_alpha)\n",
    "\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7d154-4e20-49ed-bdf9-81024bc3363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the glhymps dataset cover our domain?  NO!\n",
    "watershed_all = watersheds.exterior\n",
    "\n",
    "glhymps['geometry'] = glhymps.intersection(watershed_all.buffer(100))\n",
    "glhymps = glhymps[~(glhymps.geometry.is_empty | glhymps.geometry.isna())]\n",
    "glhymps_all = glhymps.union_all()\n",
    "\n",
    "# does the glhymps dataset cover our domain?  NO!\n",
    "print(glhymps_all.contains(watershed_all))\n",
    "\n",
    "# what fraction of the area is not covered?  VERY TINY\n",
    "missing_part = watershed_all - glhymps_all\n",
    "print('Area fraction of missing areas:', missing_part.area / watershed_all.area)\n",
    "\n",
    "# what is it? A bunch of small polygons...\n",
    "print(type(missing_part))\n",
    "print(len(missing_part.geoms))\n",
    "print(set(type(p) for p in missing_part.geoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67a571-d339-4337-a386-bb34f46c3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(glhymps.area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac86106-acc7-4db1-ba02-69b3c475e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, can we cover them up by buffering each polygon?  Note this will result in overlapping glhymps polygons, but that's ok in this case.\n",
    "def buffer_by(df : gpd.GeoDataFrame, dist : float) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Buffer each geometry in df, but only including the portion of the buffered shape that is not already in the union of all geometries.\n",
    "    \n",
    "    Note this results in overlaps where the buffers are both exterior and overlapping.\n",
    "    \"\"\"\n",
    "    df_all = df.union_all()\n",
    "    new_geometry = [shapely.union(p, p.buffer(dist) - df_all) for p in df.geometry]\n",
    "    df_copy = df.copy()\n",
    "    df_copy['geometry'] = new_geometry\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "glhymps = buffer_by(glhymps, 1000)\n",
    "glhymps_all = glhymps.union_all()\n",
    "print(glhymps_all.contains(watershed_all))\n",
    "glhymps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e52721-3eab-4916-ab2e-17ed9a151e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality check -- make sure glymps shapes cover the watershed\n",
    "assert glhymps.union_all().contains(watersheds.exterior)\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7b606-8b5c-4bbe-b123-0d8a14bbd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "glhymps.pop('logk_stdev [-]')\n",
    "\n",
    "assert glhymps['porosity [-]'][:].min() >= min_porosity\n",
    "assert glhymps['permeability [m^2]'][:].max() <= max_permeability\n",
    "assert glhymps['van Genuchten alpha [Pa^-1]'][:].max() <= max_vg_alpha\n",
    "\n",
    "glhymps.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11f6cb-57c6-4896-9f1d-571f232adfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for larger areas there are often common regions -- two labels with the same properties -- no need to duplicate those with identical values.\n",
    "def reindex_remove_duplicates(df, index):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(glhymps, 'ID')\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7b793-f2b8-459f-80ba-fb1fc37535c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert glhymps.union_all().contains(watersheds.exterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f75fe3-f365-4f54-81b0-820dc312fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the geo color of each cell of the mesh\n",
    "geology_color_glhymps = watershed_workflow.getShapePropertiesOnMesh(m2, glhymps, 'index', \n",
    "                                                         resolution=50, nodata=-999)\n",
    "\n",
    "# retain only the unique values of geology that actually appear in our cell mesh\n",
    "unique_geology_colors = list(np.unique(geology_color_glhymps))\n",
    "if -999 in unique_geology_colors:\n",
    "    unique_geology_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of geology_color\n",
    "glhymps = glhymps.loc[unique_geology_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "glhymps['ATS ID'] = range(100, 100+len(unique_geology_colors))\n",
    "glhymps['TMP_ID'] = glhymps.index\n",
    "glhymps.reset_index(drop=True, inplace=True)\n",
    "glhymps.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new geology color using the ATS IDs\n",
    "geology_color = -np.ones_like(geology_color_glhymps)\n",
    "for ats_ID, tmp_ID in zip(glhymps.index, glhymps.TMP_ID):\n",
    "    geology_color[np.where(geology_color_glhymps == tmp_ID)] = ats_ID\n",
    "\n",
    "glhymps.pop('TMP_ID')\n",
    "\n",
    "m2.cell_data['geology_color'] = geology_color\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ba4ff-b42c-494c-ac4c-b26e85800550",
   "metadata": {},
   "outputs": [],
   "source": [
    "geology_color_glhymps.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33861d6f-6e78-40b9-8975-490cdde9ad30",
   "metadata": {},
   "source": [
    "### Combine to form a complete subsurface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64069d-a8ba-4ecd-8ea4-5000d8e0c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = watershed_workflow.soil_properties.getDefaultBedrockProperties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pd.concat([glhymps, nrcs, bedrock])\n",
    "\n",
    "# save the properties to disk for use in generating input file\n",
    "subsurface_props.to_csv(toOutput('subsurface_properties', f'{name}_subsurface_properties.csv'))\n",
    "subsurface_props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5416e2-b0cf-40f5-a0ca-5ed166d7c2fe",
   "metadata": {},
   "source": [
    "## Extrude the 2D Mesh to make a 3D mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26254482-446c-4292-a102-2503d0f0676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the floor of the domain as max DTB\n",
    "dtb_max = np.nanmax(m2.cell_data['dtb'].values)\n",
    "m2.cell_data['dtb'] = m2.cell_data['dtb'].fillna(dtb_max)\n",
    "\n",
    "print(f'total thickness: {dtb_max} m')\n",
    "total_thickness = 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a206a-6004-4ff3-a588-0f0188abc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "#\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = watershed_workflow.mesh.optimizeDzs(0.05, 0.5, 2, 10)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609d8d0-5162-45a9-aaba-cf0bd5ebd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df1730-2c1b-4335-aa68-22b13f085d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50m total thickness, minus 2m soil thickness, leaves us with 48 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = watershed_workflow.mesh.optimizeDzs(1, 10, 48, 8)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "dzs_geo = [1.0, 2.0, 4.0, 8.0, 11, 11, 11]\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7215344-4bf1-4dd5-99ba-1d4022c2e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(m2.cell_data['soil_color'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bf545-40bf-4ded-919e-cdf4db7b41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer extrusion\n",
    "DTB = m2.cell_data['dtb'].values\n",
    "soil_color = m2.cell_data['soil_color'].values\n",
    "geo_color = m2.cell_data['geology_color'].values\n",
    "assert min(geo_color) > 0\n",
    "soil_thickness = m2.cell_data['soil thickness'].values\n",
    "\n",
    "\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    # use glhymps params\n",
    "    br_or_geo = np.where(depth < DTB, geo_color, 999)\n",
    "    soil_or_br_or_geo = np.where(np.bitwise_and(soil_color > 0, depth < soil_thickness),\n",
    "                                 soil_color,\n",
    "                                 br_or_geo)\n",
    "\n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    geo_or_br = np.where(depth < DTB, geo_color, 999)\n",
    "\n",
    "    layer_mat_ids.append(geo_or_br)\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "watershed_workflow.mesh.Mesh3D.summarizeExtrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)\n",
    "\n",
    "# downselect subsurface properties to only those that are used\n",
    "layer_mat_id_used = list(np.unique(np.array(layer_mat_ids)))\n",
    "print('using the following mat ids:', layer_mat_id_used)\n",
    "subsurface_props_used = subsurface_props.loc[layer_mat_id_used]\n",
    "subsurface_props_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d233a-3936-4fbf-b45c-3e19e319f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = watershed_workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b86eea-ab25-4804-b745-43bf6cb328f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2D labeled sets')\n",
    "print('---------------')\n",
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D labeled sets')\n",
    "print('------------------------')\n",
    "for ls in m3.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D side sets')\n",
    "print('---------------------')\n",
    "for ls in m3.side_sets:\n",
    "    print(f'{ls.setid} : FACE : {len(ls.cell_list)} : \"{ls.name}\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782edfa-f32d-4682-8281-6a8122285da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the mesh to disk\n",
    "output_filenames['mesh'] = toOutput('final mesh', f'{name}.exo')\n",
    "try:\n",
    "    os.remove(output_filenames['mesh'])\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "m3.writeExodus(output_filenames['mesh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a88b8-8d59-4b7b-949f-29c809a81b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labeled set info to disk as well -- just enough for use in input file (we don't need ids)\n",
    "labeled_sets = [(ls.name, ls.setid, ls.entity) for ls in m3.labeled_sets] + \\\n",
    "               [(ls.name, ls.setid, 'FACE') for ls in m3.side_sets]\n",
    "\n",
    "with open(toOutput('labeled set metadata', 'ls_metadata.pickle'), 'wb') as fid:\n",
    "    pickle.dump(labeled_sets, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2e763-8567-4f0b-9b81-48a195140fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the soil properties file\n",
    "subsurface_props_used = subsurface_props_used.reset_index(names='ats_id')\n",
    "subsurface_props_used.to_csv(toOutput('subsurface properties', '{name}_subsurface_props_used.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cb493-69b3-4624-b872-5da6acfd2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly, reread, update, and output filenames\n",
    "with open(toOutput('06_output_filenames', '06_output_filenames.txt'), 'wb') as fid:\n",
    "    pickle.dump(output_filenames, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f226b06-c1b2-4a5f-a009-707827d53338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ww-geopandas-20250725]",
   "language": "python",
   "name": "conda-env-ww-geopandas-20250725-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
